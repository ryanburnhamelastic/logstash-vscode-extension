# Complex Logstash pipeline with multiple filter plugins
# This demonstrates a real-world pipeline with various filter types

input {
  # Multiple input sources
  file {
    path => "/var/log/apache2/access.log"
    start_position => "beginning"
    tags => ["apache"]
  }
  
  beats {
    port => 5044
    host => "0.0.0.0"
    ssl => false
    tags => ["filebeat"]
  }
}

filter {
  # First split multiline logs if needed
  if "multiline" in [tags] {
    multiline {
      pattern => "^%{TIMESTAMP_ISO8601}"
      negate => true
      what => "previous"
    }
  }

  # Parse logs with different patterns based on source
  if "apache" in [tags] {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
  } else if "filebeat" in [tags] {
    grok {
      match => { "message" => "%{SYSLOGLINE}" }
    }
  }

  # Add timestamp
  date {
    match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    target => "@timestamp"
  }

  # Parse user agent if it exists
  if [agent] {
    useragent {
      source => "agent"
      target => "user_agent"
    }
  }

  # Add geoip information based on client IP
  geoip {
    source => "clientip"
    target => "geo"
  }

  # Convert fields to proper types
  mutate {
    convert => {
      "bytes" => "integer"
      "response" => "integer"
    }
    remove_field => ["message"]
  }

  # Drop health check requests to reduce noise
  if [request] =~ "^/health" or [request] =~ "^/ping" {
    drop { }
  }

  # Parse JSON fields if present
  if [body] {
    json {
      source => "body"
      target => "json_data"
    }
  }

  # Add key-value pairs from log message
  kv {
    source => "message"
    field_split => " "
    value_split => "="
    include_keys => [ "duration", "status", "method" ]
  }

  # UUID for each event
  uuid {
    target => "event_id"
  }

  # Truncate overly long fields
  truncate {
    length_bytes => 10000
    fields => ["message", "stack_trace"]
  }
}

output {
  # Route to different outputs based on tags
  if "error" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "errors-%{+YYYY.MM.dd}"
    }
    
    # Also send critical errors to email
    if [loglevel] == "CRITICAL" {
      email {
        to => "admin@example.com"
        subject => "Critical Error Detected"
        body => "Error message: %{message}"
      }
    }
  } else {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "logs-%{+YYYY.MM.dd}"
    }
  }
  
  # Archive all logs as a backup
  s3 {
    bucket => "logs-archive"
    region => "us-east-1"
    prefix => "logstash/%{+YYYY/MM/dd}"
    codec => "json_lines"
  }
} 